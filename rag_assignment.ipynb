{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e7ba33ef66cbec",
   "metadata": {},
   "source": [
    "## NLP Assignment: RAGs For Open Domain Complex QA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0589ffebd68fb9",
   "metadata": {},
   "source": [
    "Henry: stuff for training on colab, do not delete pls :D"
   ]
  },
  {
   "cell_type": "code",
   "id": "24ddb242bf4a5a4d",
   "metadata": {},
   "source": [
    "# Google colab does not save your files and will delete them when your session terminates, persist models in google drive.\n",
    "# from google.colab import drive\n",
    "# import shutil\n",
    "# drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d45834004b768af",
   "metadata": {},
   "source": [
    "# Google Colab Execution Steps\n",
    "# STEPS TO RUN ON GOOGLE COLAB\n",
    "# 1.) Select the ipynb file as your notebook\n",
    "# 2.) Change your runtime to the free tier GPU\n",
    "# upload a zip file of this project called NLPProject.zip under '/content' which is the working directory of the notebook\n",
    "# 3.) run this script\n",
    "# !rm -R sample_data/\n",
    "# !unzip NLPProject.zip\n",
    "# !rm -rf NLPProject/rag_assignment.ipynb\n",
    "# !mv NLPProject/* .\n",
    "# !rm -R NLPProject\n",
    "# !rm -rf NLPProject.zip\n",
    "# !pip install -e .\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/index.joblib','/content/indices/adore/corpus')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/dev.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/test.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/train.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/wiki_musique_corpus.json','/content/data')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d53e160eec5dde9",
   "metadata": {},
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.ADORERetriever import ADORERetriever\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore, DotScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from dexter.utils.metrics.CoverExactMatch import CoverExactMatch\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a2945406b7b8072",
   "metadata": {},
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec912aac0411e977",
   "metadata": {},
   "source": [
    "# -----------------------------------------\n",
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# #\n",
    "# # Contriever taken from huggingface\n",
    "# model_path = 'huggingface/contriever'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "# model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "#\n",
    "# # Llama taken from huggingface\n",
    "# model_name = \"huggingface/llama\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# But uncomment these two\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_path = \"facebook/contriever\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e141f620df4168",
   "metadata": {},
   "source": [
    "### Load the Dataset and set up contriever\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64a096bd42be766e",
   "metadata": {},
   "source": [
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1 if relevant\n",
    "corpus: List[Evidence]\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "# Load dataset with dev set\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "# Extract ground truth answers for the questions\n",
    "raw_data = loader.base_dataset.raw_data\n",
    "question_ground_truth_answer_map = {sample.question.id(): sample.answer.text() for sample in raw_data}\n",
    "\n",
    "# Setup contriever\n",
    "con = Contriever(config_instance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "478f3cd10eb8beda",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "id": "fabd966aef4c03b2",
   "metadata": {},
   "source": [
    "from dexter.llms.llama_engine import LlamaEngine\n",
    "# Setup LLM\n",
    "llm_instance = LlamaEngine(data=\"\", model_name=model_name, temperature=0.3, top_n=1)\n",
    "\n",
    "# Code to query llm\n",
    "def query_llm(question_text: str, evidences: List[Evidence]):\n",
    "    \"\"\"\n",
    "    :param question_text: question text\n",
    "    :param evidences: list of evidences\n",
    "    :return: the answer or None if no answer\n",
    "    \"\"\"\n",
    "    evidence_text = \"\\n\".join(doc.text() for doc in evidences)\n",
    "    system_prompt = \"Follow the given examples and Given the question and context output final answer for the question using information in the context and give answer in form of  [Final Answer]: \\n\"\n",
    "    user_prompt = f\"Question: {question_text}\\nContext: {evidence_text}\\nAnswer:\"\n",
    "\n",
    "    chain_answer = llm_instance.get_llama_completion(system_prompt, user_prompt)\n",
    "\n",
    "    # Parse the response\n",
    "    if \"not possible\" in chain_answer.lower() or \"unknown\" in chain_answer.lower():\n",
    "        return None\n",
    "    elif \"[Final Answer]:\" in chain_answer:\n",
    "        answer = chain_answer.split(\"[Final Answer]:\")[-1].strip()\n",
    "        return answer\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34d9e0f034a4b1d7",
   "metadata": {},
   "source": [
    "def evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query):\n",
    "    \"\"\"\n",
    "    Evaluates a single query by selecting relevant and non-relevant documents,\n",
    "    combining them, passing them to the LLM, and comparing the answer to the ground truth.\n",
    "\n",
    "    :param query: The query to be evaluated.\n",
    "    :param non_relevant_doc_ids_of_query: The ids of the non-relevant documents.\n",
    "    :param top_k_similar_doc_ids_of_query: The ids of the similar documents.\n",
    "    :return: A boolean ismatch indicating whether the evaluation is correct.\n",
    "    \"\"\"\n",
    "    # Fetch the actual similar documents (not just ids)\n",
    "    top_k_similar_docs_of_query = [doc for doc in corpus if doc.id() in top_k_similar_doc_ids_of_query]\n",
    "\n",
    "    # Convert selected doc IDs to actual documents\n",
    "    non_relevant_docs = []\n",
    "    if len(non_relevant_doc_ids_of_query) != 0:\n",
    "        non_relevant_docs = [doc for doc in corpus if doc.id() in non_relevant_doc_ids_of_query]\n",
    "\n",
    "    # Combine relevant and selected non-relevant docs\n",
    "    combined_docs = top_k_similar_docs_of_query + non_relevant_docs\n",
    "    random.shuffle(combined_docs)\n",
    "\n",
    "    # Evaluate the LLM answer\n",
    "    answer = query_llm(query.text(), combined_docs)\n",
    "    ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "\n",
    "    # Check if the answer matches the ground truth\n",
    "    is_match = answer is not None and ground_truth_answer.lower() in answer.lower()\n",
    "\n",
    "    # Write the result to the CSV file\n",
    "    relevant_text = \"\\n\".join(doc.text() for doc in top_k_similar_docs_of_query)\n",
    "    non_relevant_text = \"\\n\".join(doc.text() for doc in non_relevant_docs)\n",
    "    row = [query.text(), ground_truth_answer, answer, is_match, relevant_text, non_relevant_text]\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    # print(\"Answer: \", answer)\n",
    "    # print(\"Ground truth answer: \", ground_truth_answer)\n",
    "    # print(\"Is match: \", is_match)\n",
    "\n",
    "    return is_match"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e06ed2190b7ec30",
   "metadata": {},
   "source": [
    "def evaluate_with_docs(top_k, top_k_similar_docs_by_query, non_relevant_doc_ids_by_query, num_docs_to_sample):\n",
    "    \"\"\"\n",
    "    General evaluation function for both random and hard negatives. Take relevant and non-relevant docs for each query and feed them to the llm.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "\n",
    "    print(f\"There are {top_k} relevant docs and {num_docs_to_sample} non-relevant docs per query\")\n",
    "\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['Question', 'Ground Truth Answer', 'LLM Answer', 'Match', 'Relevant Documents', 'Non-Relevant Documents']\n",
    "        writer.writerow(header)\n",
    "\n",
    "    for query in tqdm(queries, desc=\"Evaluating queries\", unit=\"query\"):\n",
    "        top_k_similar_doc_ids_of_query = set(top_k_similar_docs_by_query[query.id()].keys())\n",
    "        non_relevant_doc_ids_of_query = non_relevant_doc_ids_by_query[query.id()]\n",
    "        is_match = evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query)\n",
    "        if is_match:\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "    return matches, mismatches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d33f2ec424393417",
   "metadata": {},
   "source": [
    "### Retrieve all \"relevant\" docs based on similarity"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed4d178ec2d67d71",
   "metadata": {},
   "source": [
    "def get_top_k_from_retrieved(retrieved: Dict[str, Dict[str, float]], top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract the top-k documents for each query from the retrieved results.\n",
    "    This is to avoid calling con.retrieve multiple times.\n",
    "    \"\"\"\n",
    "    top_k_results = {}\n",
    "    for query_id, docs_scores in retrieved.items():\n",
    "        sorted_docs = sorted(docs_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_results[query_id] = {doc_id: score for doc_id, score in sorted_docs[:top_k]}\n",
    "    return top_k_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1dd792a106f319d7",
   "metadata": {},
   "source": [
    "similarity_measure = CosScore()\n",
    "\n",
    "# Calculate similarities for all queries and docs\n",
    "similar_docs_by_query = con.retrieve(corpus, queries, top_k=1, score_function=similarity_measure, chunk=True, chunksize=400)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db46df4152b2813",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ]
  },
  {
   "cell_type": "code",
   "id": "611072bdb8427c94",
   "metadata": {},
   "source": [
    "k_values = [1, 3, 5]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "k = max(k_values)\n",
    "# Evaluate top_k contexts with Exact Match, use previously retrieved results\n",
    "def evaluate_with_top_k(retrieved, top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    response = get_top_k_from_retrieved(retrieved, top_k)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c654425f0294773b",
   "metadata": {},
   "source": [
    "top_k_docs_by_query: Dict[str, Dict[str, float]] = evaluate_with_top_k(similar_docs_by_query, k)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d32f80641c1ed1d",
   "metadata": {},
   "source": [
    "top_k_values = [1, 3, 5]\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    output_file = f'llm_relevant_results-{top_k}-0.csv'\n",
    "    print(f\"Running for top_k={top_k}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    non_relevant_ignore_for_here = {query.id : list() for query in queries} #compute_all_non_relevant(0, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, non_relevant_ignore_for_here, num_docs_to_sample=0)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d78c4c7b30cec297",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_query_task_2(query, orcal_contexts, sim_metric, output_file):\n",
    "    # Evaluate the LLM answer\n",
    "    answer = query_llm(query.text(), orcal_contexts)\n",
    "    ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "\n",
    "    if ground_truth_answer is None:\n",
    "        ground_truth_answer = \"None\"\n",
    "\n",
    "    # Check if the answer matches the ground truth\n",
    "    is_match = sim_metric.evaluate(ground_truth_answer, answer)\n",
    "\n",
    "    # Write the result to the CSV file\n",
    "    relevant_text = \"\\n\".join(doc.text() for doc in orcal_contexts)\n",
    "    row = [query.text(), ground_truth_answer, answer, is_match, relevant_text]\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"Ground truth answer: \", ground_truth_answer)\n",
    "    print(\"Is match: \", is_match)\n",
    "\n",
    "    return is_match\n",
    "\n",
    "def get_oracle_contexts(query, qrels, corpus) -> List[Evidence]:\n",
    "    query_id = str(query.id())  # Get the query ID as a string\n",
    "    if query_id not in qrels:\n",
    "        raise ValueError(f\"No relevant documents found for query ID {query_id} in qrels.\")\n",
    "\n",
    "    # Get relevant document IDs for the query\n",
    "    relevant_doc_ids = qrels[query_id]\n",
    "\n",
    "    # Fetch the corresponding evidence texts from the corpus\n",
    "    oracle_contexts = [corpus[int(doc_id)] for doc_id in relevant_doc_ids.keys()]\n",
    "\n",
    "    return oracle_contexts"
   ],
   "id": "5cfef01e1f2e98aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_file = f'llm_task2_orcal_results.csv'\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Question', 'Ground Truth Answer', 'LLM Answer', 'Match', 'Relevant Documents']\n",
    "    writer.writerow(header)\n",
    "\n",
    "matches = 0\n",
    "mismatches = 0\n",
    "sim_metric = CoverExactMatch()\n",
    "for query in tqdm(queries, desc=\"Evaluating queries\", unit=\"query\"):\n",
    "    oracle_contexts = get_oracle_contexts(query, qrels, corpus)\n",
    "    is_match = evaluate_query_task_2(query, oracle_contexts, sim_metric, output_file)\n",
    "    if is_match:\n",
    "        matches += 1\n",
    "    else:\n",
    "        mismatches += 1\n",
    "\n",
    "print(\"Matches:\", matches)\n",
    "print(\"Mismatches:\", mismatches)\n",
    "print(\"precision:\", matches / (matches + mismatches))"
   ],
   "id": "6cd42756a86894af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98a6afe10854b064",
   "metadata": {},
   "source": [
    "### Task 3 & 4\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff9553f87693b353",
   "metadata": {},
   "source": [
    "def compute_all_non_relevant(num_docs_to_sample, is_hard_negatives):\n",
    "    \"\"\"\n",
    "    Computes all non-relevant docs (aka random or hard negatives) for a list of queries and a corpus.\n",
    "    \"\"\"\n",
    "    all_non_relevant = {}\n",
    "\n",
    "    for query in tqdm(queries, desc=f\"Retrieving {num_docs_to_sample} non-relevant docs for each query\", unit=\"query\"):\n",
    "        query_id = query.id()\n",
    "\n",
    "        # Extract non-relevant document IDs based on qrels for this query\n",
    "        if num_docs_to_sample == 0:\n",
    "            all_non_relevant[query_id] = list()\n",
    "            continue\n",
    "        non_relevant_doc_ids = {doc.id() for doc in corpus if not (doc.id() in qrels[query_id])}\n",
    "\n",
    "        if is_hard_negatives:\n",
    "            most_similar_docs = get_top_k_from_retrieved(similar_docs_by_query, 100)\n",
    "            # Get similar and nonrelevant docs, remove the ones with similarity 0\n",
    "            similar_and_non_relevant_doc_ids = {doc_id for doc_id in most_similar_docs[query_id] if most_similar_docs[query_id][doc_id] > 0 and doc_id in non_relevant_doc_ids}\n",
    "            non_relevant_doc_ids = similar_and_non_relevant_doc_ids\n",
    "\n",
    "        if num_docs_to_sample > len(non_relevant_doc_ids):\n",
    "            print(\"Not enough documents to sample from, please select smaller ratio or reduce the number of similar docs per query to select\")\n",
    "            return None\n",
    "        all_non_relevant[query_id] = list(non_relevant_doc_ids)[:num_docs_to_sample]\n",
    "\n",
    "    return all_non_relevant\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "806eb39c666f30a1",
   "metadata": {},
   "source": [
    "# Try with only relevant contexts\n",
    "samples_values = [12, 24]\n",
    "for top_k in samples_values:\n",
    "    output_file = f'llm_relevant_results-{top_k}-0.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_other_docs_to_sample=0\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    other_doc_ids_by_query = compute_all_non_relevant(0, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, other_doc_ids_by_query, num_docs_to_sample=0)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86abe77e43f122bd",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "id": "101f766617d34fe7",
   "metadata": {},
   "source": [
    "# Try with only random contexts\n",
    "samples_values = [12, 24]\n",
    "top_k = 0\n",
    "for num_random_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, random_doc_ids_by_query, num_random_docs_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "442c6d94b1c7511e",
   "metadata": {},
   "source": [
    "# Pairs of total 12: 3+9, 6+6, 9+3\n",
    "top_k_values = [3, 6, 9]\n",
    "samples_values = [9, 6, 3]\n",
    "# Evaluate with random context\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, random_doc_ids_by_query, num_random_docs_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab8438495ab65712",
   "metadata": {},
   "source": [
    "# Pairs of total 24: 6+18, 12+12, 18+6\n",
    "top_k_values = [6, 12, 18]\n",
    "samples_values = [18, 12, 6]\n",
    "# Evaluate with random context\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, random_doc_ids_by_query, num_random_docs_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a15e917c819a7377",
   "metadata": {},
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ]
  },
  {
   "cell_type": "code",
   "id": "dc9acda8217bb816",
   "metadata": {},
   "source": [
    "# Add only hard negatives\n",
    "# Try with only contexts\n",
    "samples_values = [12, 24]\n",
    "top_k = 0\n",
    "for num_hard_neg_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_neg_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_neg_docs_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_neg_docs_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, hard_negative_doc_ids_by_query, num_hard_neg_docs_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "268a8d9d73a80327",
   "metadata": {},
   "source": [
    "# Hard negatives are the documents both in retrieved docs (they are similar) and non-relevant docs (since they are not actually relevant)\n",
    "# Evaluate with hard negative contexts\n",
    "# Pairs of total 12: 3+9, 6+6, 9+3\n",
    "top_k_values = [3, 6, 9]\n",
    "samples_values = [9, 6, 3]\n",
    "\n",
    "for top_k, num_hard_negatives_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_negatives_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, hard_negative_doc_ids_by_query, num_hard_negatives_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb0271cec6337f05",
   "metadata": {},
   "source": [
    "# Pairs of total 24: 6+18, 12+12, 18+6\n",
    "top_k_values = [6, 12, 18]\n",
    "samples_values = [18, 12, 6]\n",
    "for top_k, num_hard_negatives_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_negatives_to_sample}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, hard_negative_doc_ids_by_query, num_hard_negatives_to_sample)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e0355ff42a80585",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ]
  },
  {
   "cell_type": "code",
   "id": "50cf067337688dfe",
   "metadata": {},
   "source": [
    "adore_config = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32\n",
    "                                    ,learning_rate=5e-6)\n",
    "# Setup contriever\n",
    "adore_retriever = ADORERetriever(adore_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d56e61c6bbb6d60e",
   "metadata": {},
   "source": [
    "adore_retriever.train(queries=queries, corpus=corpus, qrels=qrels, top_k=50, n_epochs=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "251dab90d239bf78",
   "metadata": {},
   "source": [
    "adore_retriever.save_query_encoder(\"results\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15f0414360538302",
   "metadata": {},
   "source": [
    "# THIS MUST BE THE SAME AS THE SIMILARITY METRIC BEING USED IN ADORERETRIEVER.PY\n",
    "similarity_measure = CosScore()\n",
    "adore_retriever.load_query_encoder(\"results\")\n",
    "\n",
    "con.question_encoder = adore_retriever.question_encoder\n",
    "con.question_encoder.to('cuda')\n",
    "similar_docs_by_query_adore = con.retrieve(corpus, queries, top_k=100, score_function=similarity_measure, chunk=True, chunksize=400000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7144fe06038b27e5",
   "metadata": {},
   "source": [
    "top_k_docs_by_query_adore = evaluate_with_top_k(similar_docs_by_query_adore, k)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1850556079bc99e6",
   "metadata": {},
   "source": [
    "top_k_values = [1, 3, 5]\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    output_file = f'llm_adore_results-{top_k}-0.csv'\n",
    "    print(f\"Running for top_k={top_k}\")\n",
    "\n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query_adore, top_k)\n",
    "    non_relevant_ignore_for_here = compute_all_non_relevant(0, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, top_k_similar_docs_by_query, non_relevant_ignore_for_here, 0)\n",
    "\n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6dd6d75390ac0e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
