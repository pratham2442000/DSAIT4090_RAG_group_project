{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd8a92acdf73e00",
   "metadata": {},
   "source": [
    "## NLP Assignment: RAGs For Open Domain Complex QA\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a07a59e9fd150a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:34:35.924571Z",
     "start_time": "2024-12-24T16:34:26.740225Z"
    }
   },
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import csv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find CUDA.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a50e93acf28a226e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:34:35.955077Z",
     "start_time": "2024-12-24T16:34:35.925569Z"
    }
   },
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "55423df49501736c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:34:36.624740Z",
     "start_time": "2024-12-24T16:34:35.955077Z"
    }
   },
   "source": [
    "# -----------------------------------------\n",
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Contriever taken from huggingface\n",
    "model_path = 'huggingface/contriever'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Llama taken from huggingface\n",
    "model_name = \"huggingface/llama\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# But uncomment these two\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_path = \"facebook/contriever\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "8719d6cb521f8539",
   "metadata": {},
   "source": "### Load the Dataset and set up contriever\n"
  },
  {
   "cell_type": "code",
   "id": "3a7c486097b42d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:36:11.360229Z",
     "start_time": "2024-12-24T16:34:36.625906Z"
    }
   },
   "source": [
    "from dexter.llms.llama_engine import LlamaEngine\n",
    "\n",
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1/0 true false if relevant\n",
    "corpus: List[Evidence]\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "# Load dataset with dev set\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "# Extract ground truth answers for the questions\n",
    "raw_data = loader.base_dataset.raw_data\n",
    "question_ground_truth_answer_map = {sample.question.id(): sample.answer.text() for sample in raw_data}\n",
    "\n",
    "# Setup contriever\n",
    "con = Contriever(config_instance)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1762967.49it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:00<00:00, 661839.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:31<00:00, 13.12it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LLM",
   "id": "24917b889cc8071e"
  },
  {
   "cell_type": "code",
   "id": "25291f1e31a6f77d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:36:15.883961Z",
     "start_time": "2024-12-24T16:36:11.360632Z"
    }
   },
   "source": [
    "# Setup LLM\n",
    "llm_instance = LlamaEngine(data=\"\", model_name=model_name, temperature=0.3, top_n=1)\n",
    "\n",
    "# Code to query llm\n",
    "def query_llm(question_text: str, evidences: List[Evidence]):\n",
    "    \"\"\"\n",
    "    :param question_text: question text\n",
    "    :param evidences: list of evidences\n",
    "    :return: the answer or None if no answer\n",
    "    \"\"\"\n",
    "    evidence_text = \"\\n\".join(doc.text() for doc in evidences)\n",
    "    system_prompt = \"Follow the given examples and Given the question and context output final answer for the question using information in the context and give answer in form of  [Final Answer]: \\n\"\n",
    "    user_prompt = f\"Question: {question_text}\\nContext: {evidence_text}\\nAnswer:\"\n",
    "\n",
    "    chain_answer = llm_instance.get_llama_completion(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the response\n",
    "    if \"not possible\" in chain_answer.lower() or \"unknown\" in chain_answer.lower():\n",
    "        return None\n",
    "    elif \"[Final Answer]:\" in chain_answer:\n",
    "        answer = chain_answer.split(\"[Final Answer]:\")[-1].strip()\n",
    "        return answer\n",
    "        \n",
    "    return None"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f2729025d784bdbb69172023263346f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a7474923e9649fdabc52c43c92568da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieve all \"relevant\" docs based on similarity",
   "id": "2cfccb7bcad45819"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:36:15.899324Z",
     "start_time": "2024-12-24T16:36:15.883961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_from_retrieved(retrieved: Dict[str, Dict[str, float]], top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract the top-k documents for each query from the retrieved results.\n",
    "    This is to avoid calling con.retrieve multiple times.\n",
    "    \"\"\"\n",
    "    top_k_results = {}\n",
    "    for query_id, docs_scores in retrieved.items():\n",
    "        sorted_docs = sorted(docs_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_results[query_id] = {doc_id: score for doc_id, score in sorted_docs[:top_k]}\n",
    "    return top_k_results"
   ],
   "id": "2c0d1181ea548c02",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.515288Z",
     "start_time": "2024-12-24T16:36:15.899324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "\n",
    "# Calculate similarities for all queries and docs\n",
    "similar_docs_by_query = con.retrieve(corpus, queries, top_k=100, score_function=similarity_measure, chunk=True, chunksize=400000)"
   ],
   "id": "171d35299443e5ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [14:07<00:00, 472.23it/s]\n",
      "100%|██████████| 163424/163424 [05:46<00:00, 471.79it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "813e47789013683b",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.562577Z",
     "start_time": "2024-12-24T16:56:10.516358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Evaluate top_k contexts with Exact Match, use previously retrieved results\n",
    "def evaluate_with_top_k(retrieved, top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    response = get_top_k_from_retrieved(retrieved, top_k)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response\n",
    "\n",
    "k = max(k_values)\n",
    "top_k_docs_by_query: Dict[str, Dict[str, float]] = evaluate_with_top_k(similar_docs_by_query, k)"
   ],
   "id": "567a36ecc4fc5fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices retrieved: 1200\n",
      "({'NDCG@1': 0.42417, 'NDCG@2': 0.3858, 'NDCG@3': 0.34182}, {'MAP@1': 0.04266, 'MAP@2': 0.0646, 'MAP@3': 0.0762}, {'Recall@1': 0.04266, 'Recall@2': 0.07536, 'Recall@3': 0.09527}, {'P@1': 0.42417, 'P@2': 0.37458, 'P@3': 0.31583})\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d2e8ff89cee63d0b",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ]
  },
  {
   "cell_type": "code",
   "id": "99138e5d402cc642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.577811Z",
     "start_time": "2024-12-24T16:56:10.562814Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "62297b9db0795fd",
   "metadata": {},
   "source": [
    "### Task 3 & 4\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.593757Z",
     "start_time": "2024-12-24T16:56:10.579804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query):\n",
    "    \"\"\"\n",
    "    Evaluates a single query by selecting relevant and non-relevant documents,\n",
    "    combining them, passing them to the LLM, and comparing the answer to the ground truth.\n",
    "    \n",
    "    :param query: The query to be evaluated.\n",
    "    :param non_relevant_doc_ids_of_query: The ids of the non-relevant documents.\n",
    "    :param top_k_similar_doc_ids_of_query: The ids of the similar documents.\n",
    "    :return: A boolean ismatch indicating whether the evaluation is correct.\n",
    "    \"\"\"\n",
    "    # Fetch the actual similar documents (not just ids)\n",
    "    top_k_similar_docs_of_query = [doc for doc in corpus if doc.id() in top_k_similar_doc_ids_of_query]\n",
    "    \n",
    "    # Convert selected doc IDs to actual documents\n",
    "    non_relevant_docs = [doc for doc in corpus if doc.id() in non_relevant_doc_ids_of_query]\n",
    "    \n",
    "    # Combine relevant and selected non-relevant docs\n",
    "    combined_docs = top_k_similar_docs_of_query + non_relevant_docs\n",
    "    random.shuffle(combined_docs)\n",
    "    \n",
    "    # Evaluate the LLM answer\n",
    "    answer = query_llm(query.text(), combined_docs)\n",
    "    ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "    \n",
    "    # Check if the answer matches the ground truth\n",
    "    is_match = answer is not None and ground_truth_answer.lower() in answer.lower()\n",
    "\n",
    "    # Write the result to the CSV file\n",
    "    relevant_text = \"\\n\".join(doc.text() for doc in top_k_similar_docs_of_query)\n",
    "    non_relevant_text = \"\\n\".join(doc.text() for doc in non_relevant_docs)\n",
    "    row = [query.text(), ground_truth_answer, answer, is_match, relevant_text, non_relevant_text]\n",
    "    with open(\"llm_results.csv\", mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # print(\"Answer: \", answer)\n",
    "    # print(\"Ground truth answer: \", ground_truth_answer)\n",
    "    # print(\"Is match: \", is_match)\n",
    "    \n",
    "    return is_match"
   ],
   "id": "ec3f5e86e0841065",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.609232Z",
     "start_time": "2024-12-24T16:56:10.594753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_all_non_relevant(num_docs_to_sample, is_hard_negatives):\n",
    "    \"\"\"\n",
    "    Computes all non-relevant docs (aka random or hard negatives) for a list of queries and a corpus.\n",
    "    \"\"\"\n",
    "    all_non_relevant = {}\n",
    "\n",
    "    for query in tqdm(queries, desc=f\"Retrieving {num_docs_to_sample} non-relevant docs for each query\", unit=\"query\"):\n",
    "        query_id = query.id()\n",
    "\n",
    "        # Extract non-relevant document IDs based on qrels for this query\n",
    "        non_relevant_doc_ids = {doc.id() for doc in corpus if not (doc.id() in qrels[query_id])}\n",
    "        \n",
    "        if is_hard_negatives:\n",
    "            most_similar_docs = get_top_k_from_retrieved(similar_docs_by_query, 100)\n",
    "            # Get similar and nonrelevant docs, remove the ones with similarity 0\n",
    "            similar_and_non_relevant_doc_ids = {doc_id for doc_id in most_similar_docs[query_id] if most_similar_docs[query_id][doc_id] > 0 and doc_id in non_relevant_doc_ids}\n",
    "            non_relevant_doc_ids = similar_and_non_relevant_doc_ids\n",
    "            # sampled_non_relevant_ids = random.sample(non_relevant_doc_ids, num_docs_to_sample*5)  # This is a bit random\n",
    "            # sampled_docs = [doc for doc in corpus if doc.id() in sampled_non_relevant_ids]\n",
    "            # \n",
    "            # # Compute embeddings and cosine similarity for the sampled documents\n",
    "            # query_embedding = con.encode_queries([query], batch_size=con.batch_size)  # Shape: [1, D] - encode query\n",
    "            # non_relevant_embeddings = con.encode_corpus(sampled_docs)\n",
    "            # cos_scores = similarity_measure.evaluate(query_embedding, non_relevant_embeddings)  # Shape: [1, sampled_size*5]\n",
    "            # \n",
    "            # # Select documents with similarity > 0 (hard negatives: non-relevant but do have some similarity)\n",
    "            # non_relevant_doc_ids = [doc.id() for doc, score in zip(sampled_docs, cos_scores[0].tolist()) if score > 0]\n",
    "\n",
    "        if num_docs_to_sample > len(non_relevant_doc_ids):\n",
    "            print(\"Not enough documents to sample from, please select smaller ratio or reduce the number of similar docs per query to select\")\n",
    "            return None\n",
    "        all_non_relevant[query_id] = list(non_relevant_doc_ids)[:num_docs_to_sample]\n",
    "\n",
    "    return all_non_relevant\n"
   ],
   "id": "687cb964bb805e0e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.624769Z",
     "start_time": "2024-12-24T16:56:10.610229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_docs(top_k, non_relevant_doc_ids_by_query, num_docs_to_sample):\n",
    "    \"\"\"\n",
    "    General evaluation function for both random and hard negatives.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    print(f\"There are {top_k} relevant docs and {num_docs_to_sample} non-relative docs per query\")\n",
    "    \n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['Question', 'Ground Truth Answer', 'LLM Answer', 'Match', 'Relevant Documents', 'Non-Relevant Documents']\n",
    "        writer.writerow(header)\n",
    "    \n",
    "    for query in tqdm(queries, desc=\"Evaluating queries\", unit=\"query\"):\n",
    "        top_k_similar_doc_ids_of_query = set(top_k_similar_docs_by_query[query.id()].keys())\n",
    "        non_relevant_doc_ids_of_query = non_relevant_doc_ids_by_query[query.id()]\n",
    "        is_match = evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query)\n",
    "        if is_match:\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "    return matches, mismatches"
   ],
   "id": "28b5e803d0114d8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:10.639778Z",
     "start_time": "2024-12-24T16:56:10.624769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pairs: 3+9, 6+6, 9+3 or 6+18, 12+12, 18+6\n",
    "top_k_values = [3, 6, 9, 6, 12, 18]\n",
    "samples_values = [9, 6, 3, 18, 12, 6]"
   ],
   "id": "5ca98d3e3910917e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 3",
   "id": "ea769656e45e19af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T22:28:45.168236Z",
     "start_time": "2024-12-24T16:56:10.640776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate with random context\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "a2e7eb386ee84b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=3, num_random_docs_to_sample=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 9 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:21<00:00,  8.46query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 relevant docs and 9 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   1%|          | 10/1200 [00:20<36:25,  1.84s/query]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating queries: 100%|██████████| 1200/1200 [53:13<00:00,  2.66s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 627\n",
      "Mismatches: 573\n",
      "Running for top_k=6, num_random_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:18<00:00,  8.68query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [50:35<00:00,  2.53s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 652\n",
      "Mismatches: 548\n",
      "Running for top_k=9, num_random_docs_to_sample=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 3 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:13<00:00,  8.97query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 relevant docs and 3 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [45:47<00:00,  2.29s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 682\n",
      "Mismatches: 518\n",
      "Running for top_k=6, num_random_docs_to_sample=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 18 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:18<00:00,  8.69query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 18 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [1:04:49<00:00,  3.24s/query]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 680\n",
      "Mismatches: 520\n",
      "Running for top_k=12, num_random_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:14<00:00,  8.94query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:  73%|███████▎  | 879/1200 [38:09<10:01,  1.87s/query]  This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Evaluating queries: 100%|██████████| 1200/1200 [52:20<00:00,  2.62s/query]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 708\n",
      "Mismatches: 492\n",
      "Running for top_k=18, num_random_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:14<00:00,  8.93query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [52:08<00:00,  2.61s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 706\n",
      "Mismatches: 494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Try with only contexts\n",
    "samples_values = [12, 24, 40]\n",
    "top_k = 0\n",
    "for num_random_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "24b132b4f3df4bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Try with total objects=40 and more ratios\n",
    "top_k_values = [4, 8, 12, 16, 20, 24, 28, 32, 36]\n",
    "samples_values = [36, 32, 28, 24, 20, 16, 12, 8]\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "f10112e199553525"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ],
   "id": "80d6f5137f5a5828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T03:15:35.486183Z",
     "start_time": "2024-12-24T22:28:45.169403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hard negatives are the documents both in retrieved docs (they are similar) and non-relevant docs (since they are not actually relevant)\n",
    "# Evaluate with hard negative contexts\n",
    "# Pairs: 3+9, 6+6, 9+3 or 6+18, 12+12, 18+6\n",
    "top_k_values = [3, 6, 9, 6, 12, 18]\n",
    "samples_values = [9, 6, 3, 18, 12, 6]\n",
    "\n",
    "for top_k, num_hard_negatives_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_negatives_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_negatives_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "582d7aa98394eeb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=3, num_hard_neg_docs_to_sample=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 9 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.72query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 relevant docs and 9 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [43:27<00:00,  2.17s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 639\n",
      "Mismatches: 561\n",
      "Running for top_k=6, num_hard_neg_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.71query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [42:59<00:00,  2.15s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 663\n",
      "Mismatches: 537\n",
      "Running for top_k=9, num_hard_neg_docs_to_sample=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 3 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.72query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 relevant docs and 3 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [41:51<00:00,  2.09s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 679\n",
      "Mismatches: 521\n",
      "Running for top_k=6, num_hard_neg_docs_to_sample=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 18 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.71query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 18 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [48:03<00:00,  2.40s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 641\n",
      "Mismatches: 559\n",
      "Running for top_k=12, num_hard_neg_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:34<00:00,  7.75query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [47:35<00:00,  2.38s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 695\n",
      "Mismatches: 505\n",
      "Running for top_k=18, num_hard_neg_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.73query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [47:21<00:00,  2.37s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 703\n",
      "Mismatches: 497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add only hard negatives\n",
    "# Try with only contexts\n",
    "samples_values = [12, 24, 40]\n",
    "top_k = 0\n",
    "for num_hard_neg_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_neg_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_neg_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_neg_docs_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_neg_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "7dbbb70f2ad541ff"
  },
  {
   "cell_type": "markdown",
   "id": "bb93e8c6a3e03131",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b87a7ff1f833c9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:13.851897Z",
     "start_time": "2024-12-13T10:36:13.847910Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe5da82911b029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
