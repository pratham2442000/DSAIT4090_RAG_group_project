{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Assignment: RAGs For Open Domain Complex QA\n",
   "id": "7fd8a92acdf73e00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:17:21.757369Z",
     "start_time": "2024-12-13T18:17:21.748633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.utils.metrics.CoverExactMatch import CoverExactMatch\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import json\n",
    "import torch\n",
    "import random"
   ],
   "id": "a07a59e9fd150a58",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:02:48.139325Z",
     "start_time": "2024-12-11T11:02:48.125372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "id": "a50e93acf28a226e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:51:07.653181Z",
     "start_time": "2024-12-13T10:51:06.814032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = '../contriever'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "#model_path = \"facebook/contriever\" <- but uncomment this"
   ],
   "id": "55423df49501736c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ],
   "id": "813e47789013683b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:04:42.748015Z",
     "start_time": "2024-12-13T11:02:52.234656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.llms.openai_engine import OpenAIEngine\n",
    "from dexter.llms.llm_engine_orchestrator import LLMEngineOrchestrator\n",
    "from dexter.data.loaders.WikiMultihopQADataLoader import WikiMultihopQADataLoader\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "result = loader.qrels()\n",
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1/0 true false (i think) if relevant\n",
    "corpus: List[Evidence]\n",
    "queries, qrels, corpus = result\n",
    "con = Contriever(config_instance)\n",
    "\n",
    "# Setup LLM\n",
    "\n",
    "config_instance = LLMEngineOrchestrator()\n",
    "llm_instance = config_instance.get_llm_engine(data=\"\",llm_class=\"openai\",model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# Things to consider below:\n",
    "\n",
    "# loader = WikiMultihopQADataLoader(\"wikimultihopqa\", config_path=\"est_config.ini\", split=Split.DEV, batch_size=10)\n",
    "# assert len(loader.raw_data) == len(loader.dataset)\n",
    "# self.assertTrue(isinstance(loader.dataset, DprDataset))\n",
    "# self.assertTrue(isinstance(loader.tokenizer, Tokenizer))\n",
    "# self.assertTrue(isinstance(loader.raw_data[0], Sample))\n",
    "# self.assertTrue(isinstance(loader.raw_data[0].evidences, Evidence))\n",
    "\n",
    "## wikimultihop <- it had this in the google colab notebook, should we use it?\n",
    "# with open(\"data/wiki_musique_corpus.json\") as f:\n",
    "#     corpus = json.load(f)"
   ],
   "id": "9d99959d04c1494",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1574308.94it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:00<00:00, 613485.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:46<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:30:42.520959Z",
     "start_time": "2024-12-13T11:04:42.749015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Retrieve top-k contexts and evaluate with Exact Match\n",
    "def evaluate_with_top_k(k_values: List[int]) -> Dict[str, Dict[str, float]]:\n",
    "    response = con.retrieve(corpus, queries, top_k=max(k_values), score_function=similarity_measure)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response\n",
    "\n",
    "response: Dict[str, Dict[str, float]] = evaluate_with_top_k(k_values)"
   ],
   "id": "964620ce640dc3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb torch.Size([1200, 35, 768])\n",
      "sentence_emb torch.Size([1200, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 64/563424 [00:00<18:24, 510.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding of contexts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563424/563424 [25:59<00:00, 361.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_embeddings torch.Size([563424, 768])\n",
      "Indices retrieved: 1200\n",
      "({'NDCG@1': 0.425, 'NDCG@2': 0.38567, 'NDCG@3': 0.33839}, {'MAP@1': 0.04277, 'MAP@2': 0.06457, 'MAP@3': 0.07541}, {'Recall@1': 0.04277, 'Recall@2': 0.07528, 'Recall@3': 0.09379}, {'P@1': 0.425, 'P@2': 0.37417, 'P@3': 0.31083})\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ],
   "id": "d2e8ff89cee63d0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:35:26.738290Z",
     "start_time": "2024-12-13T10:35:26.724335Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "99138e5d402cc642",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ],
   "id": "62297b9db0795fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:16:44.808439Z",
     "start_time": "2024-12-14T23:16:44.671628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.llms.llm_engine_orchestrator import LLMEngineOrchestrator\n",
    "config_instance = LLMEngineOrchestrator()\n",
    "llm_instance = config_instance.get_llm_engine(data=\"\",llm_class=\"openai\",model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "def evaluate_with_random_contexts(relevant_docs_by_query, relevant_ratio=0.5):\n",
    "    results = {}\n",
    "    i=0\n",
    "    for query in queries:\n",
    "        query_id = query.id()\n",
    "        top_k_relevant_docs_ids = list(relevant_docs_by_query[query_id].keys())\n",
    "        top_k_relevant_docs = [evidence for evidence in corpus if evidence.id in top_k_relevant_docs_ids]\n",
    "        \n",
    "        # Get non relevant docs from qrels (where qrels[query_id][doc_id] = 0)\n",
    "        non_relevant_doc_ids = {doc_id for doc_id, relevance in qrels.get(query_id, {}).items() if relevance == 0}\n",
    "        random_non_relevant_docs = [evidence for evidence in corpus if evidence.id in non_relevant_doc_ids]\n",
    "        num_random_docs = int(len(top_k_relevant_docs) * (1 - relevant_ratio) / relevant_ratio)\n",
    "        # sampled_docs = random_non_relevant_docs[:num_random_docs]\n",
    "        random_docs = random.sample(random_non_relevant_docs, min(num_random_docs, len(non_relevant_doc_ids)))\n",
    "        \n",
    "        # Combine relevant and random non-relevant docs\n",
    "        combined_docs = top_k_relevant_docs + random_docs\n",
    "        \n",
    "        # Pass to LLM for evaluation\n",
    "        docs_as_string = \"\\n\".join([doc.text for doc in combined_docs])\n",
    "        answer = llm_instance.get_chat_completion(query.text, docs_as_string)\n",
    "        # print(answer)\n",
    "        if i < 5:\n",
    "            print(answer)\n",
    "            i+=1\n",
    "        \n",
    "        # Evaluate with Exact Match\n",
    "        # results[query_id] = None\n",
    "\n",
    "evaluate_with_random_contexts(response)"
   ],
   "id": "25639be0d7fa46ad",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'huggingface_token'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdexter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllm_engine_orchestrator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LLMEngineOrchestrator\n\u001B[0;32m      2\u001B[0m config_instance \u001B[38;5;241m=\u001B[39m LLMEngineOrchestrator()\n\u001B[0;32m      3\u001B[0m llm_instance \u001B[38;5;241m=\u001B[39m config_instance\u001B[38;5;241m.\u001B[39mget_llm_engine(data\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,llm_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m,model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\Danae_temp\\NLP_RAG\\dexter\\llms\\llm_engine_orchestrator.py:9\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdexter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mflant5_engine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FlanT5Engine\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdexter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllama_engine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LlamaEngine\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdexter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmistral_engine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MistralEngine\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdexter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mopenai_engine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAIEngine\n",
      "File \u001B[1;32m~\\Desktop\\Danae_temp\\NLP_RAG\\dexter\\llms\\llama_engine.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m login\n\u001B[1;32m----> 6\u001B[0m access_token_read \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menviron\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuggingface_token\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m      7\u001B[0m login(token \u001B[38;5;241m=\u001B[39m access_token_read)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLlamaEngine\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Users\\Public\\.conda\\envs\\nlp_project\\lib\\os.py:680\u001B[0m, in \u001B[0;36m_Environ.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    677\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodekey(key)]\n\u001B[0;32m    678\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;66;03m# raise KeyError with the original key value\u001B[39;00m\n\u001B[1;32m--> 680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecodevalue(value)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'huggingface_token'"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ],
   "id": "4e75723788b2bd77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:10.266634Z",
     "start_time": "2024-12-13T10:36:10.249692Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "90f91ef62c7a9019",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ],
   "id": "bb93e8c6a3e03131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:13.851897Z",
     "start_time": "2024-12-13T10:36:13.847910Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b87a7ff1f833c9a7",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cebe5da82911b029"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
