{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Assignment: RAGs For Open Domain Complex QA\n",
   "id": "7fd8a92acdf73e00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:02:48.123754Z",
     "start_time": "2024-12-11T11:02:48.111288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.utils.metrics.CoverExactMatch import CoverExactMatch\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import json\n",
    "import torch\n",
    "import random"
   ],
   "id": "a07a59e9fd150a58",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:02:48.139325Z",
     "start_time": "2024-12-11T11:02:48.125372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "id": "a50e93acf28a226e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:02:48.621405Z",
     "start_time": "2024-12-11T11:02:48.140322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = '../contriever'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "#model_path = \"facebook/contriever\" <- but uncomment this"
   ],
   "id": "55423df49501736c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ],
   "id": "813e47789013683b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:04:30.366594Z",
     "start_time": "2024-12-11T11:02:53.212150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "con = Contriever(config_instance)\n",
    "\n",
    "## wikimultihop <- it had this in the google colab notebook, should we use it?\n",
    "# with open(\"data/wiki_musique_corpus.json\") as f:\n",
    "#     corpus = json.load(f)"
   ],
   "id": "9d99959d04c1494",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1658648.83it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 461275.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:32<00:00, 12.94it/s]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T11:31:55.833712Z",
     "start_time": "2024-12-11T11:05:36.201756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Retrieve top-k contexts and evaluate with Exact Match\n",
    "def evaluate_with_top_k(queries, corpus, k_values):\n",
    "    response = con.retrieve(corpus, queries, top_k=max(k_values), score_function=similarity_measure)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "\n",
    "evaluate_with_top_k(queries, corpus, k_values)"
   ],
   "id": "964620ce640dc3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb torch.Size([1200, 35, 768])\n",
      "sentence_emb torch.Size([1200, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/563424 [00:00<45:33, 206.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding of contexts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563424/563424 [26:18<00:00, 356.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_embeddings torch.Size([563424, 768])\n",
      "Indices retrieved: 1200\n",
      "({'NDCG@1': 0.425, 'NDCG@2': 0.38567, 'NDCG@3': 0.33839}, {'MAP@1': 0.04277, 'MAP@2': 0.06457, 'MAP@3': 0.07541}, {'Recall@1': 0.04277, 'Recall@2': 0.07528, 'Recall@3': 0.09379}, {'P@1': 0.425, 'P@2': 0.37417, 'P@3': 0.31083})\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ],
   "id": "d2e8ff89cee63d0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:35:26.738290Z",
     "start_time": "2024-12-13T10:35:26.724335Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "99138e5d402cc642",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ],
   "id": "62297b9db0795fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:35:30.900010Z",
     "start_time": "2024-12-13T10:35:30.895092Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "25639be0d7fa46ad",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ],
   "id": "4e75723788b2bd77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:10.266634Z",
     "start_time": "2024-12-13T10:36:10.249692Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "90f91ef62c7a9019",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ],
   "id": "bb93e8c6a3e03131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:13.851897Z",
     "start_time": "2024-12-13T10:36:13.847910Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b87a7ff1f833c9a7",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cebe5da82911b029"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
