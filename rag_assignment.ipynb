{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Assignment: RAGs For Open Domain Complex QA\n",
   "id": "7fd8a92acdf73e00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T12:01:25.736632Z",
     "start_time": "2024-12-18T12:01:22.694116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import torch\n",
    "import random"
   ],
   "id": "a07a59e9fd150a58",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T12:01:25.768036Z",
     "start_time": "2024-12-18T12:01:25.737630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "id": "a50e93acf28a226e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T12:01:26.761090Z",
     "start_time": "2024-12-18T12:01:25.769032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------\n",
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Contriever taken from huggingface\n",
    "model_path = 'huggingface/contriever'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Llama taken from huggingface\n",
    "model_name = \"huggingface/llama\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# But uncomment these two\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_path = \"facebook/contriever\""
   ],
   "id": "55423df49501736c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\.conda\\envs\\nlp_project\\lib\\site-packages\\transformers\\modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the Dataset and setup LLM\n",
   "id": "8719d6cb521f8539"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T12:03:02.162641Z",
     "start_time": "2024-12-18T12:01:26.761529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dexter.llms.llama_engine import LlamaEngine\n",
    "from dexter.llms.llm_engine_orchestrator import LLMEngineOrchestrator\n",
    "\n",
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1/0 true false if relevant\n",
    "corpus: List[Evidence]\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "# Load dataset with dev set\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "result = loader.qrels()\n",
    "queries, qrels, corpus = result\n",
    "raw_data = loader.base_dataset.raw_data\n",
    "\n",
    "question_ground_truth_answer_map = {sample.question.id(): sample.answer.text() for sample in raw_data}\n",
    "\n",
    "\n",
    "# Setup contriever\n",
    "con = Contriever(config_instance)\n",
    "\n",
    "# Things to consider below on how to get the answers to the questions\n",
    "\n",
    "# loader = WikiMultihopQADataLoader(\"wikimultihopqa\", config_path=\"est_config.ini\", split=Split.DEV, batch_size=10)\n",
    "# assert len(loader.raw_data) == len(loader.dataset)\n",
    "# self.assertTrue(isinstance(loader.dataset, DprDataset))\n",
    "# self.assertTrue(isinstance(loader.tokenizer, Tokenizer))\n",
    "# self.assertTrue(isinstance(loader.raw_data[0], Sample))\n",
    "# self.assertTrue(isinstance(loader.raw_data[0].evidences, Evidence))\n"
   ],
   "id": "3a7c486097b42d53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1710333.73it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:00<00:00, 589884.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:31<00:00, 13.11it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T12:44:48.392327Z",
     "start_time": "2024-12-18T12:44:06.170311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup LLM\n",
    "config_instance = LLMEngineOrchestrator()\n",
    "llm_instance = LlamaEngine(data=\"\", model_name=model_name, temperature=0.3, top_n=1)\n",
    "\n",
    "# Code to query llm\n",
    "def query_llm(question_text: str, evidences: List[Evidence]):\n",
    "    \"\"\"\n",
    "    :param question_text: question text\n",
    "    :param evidences: list of evidences\n",
    "    :return: the answer or None if no answer\n",
    "    \"\"\"\n",
    "    evidence_text = \" \".join(doc.text for doc in evidences)\n",
    "    system_prompt = \"Follow the given examples and Given the question and context output final answer for the question using information in the context and give answer in form of  [Final Answer]: \\n\"\n",
    "    user_prompt = f\"Question: {question_text}\\nContext: {evidence_text}\\nAnswer:\"\n",
    "\n",
    "    chain_answer = llm_instance.get_llama_completion(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the response\n",
    "    if \"not possible\" in chain_answer.lower() or \"unknown\" in chain_answer.lower():\n",
    "        return None\n",
    "    elif \"[Final Answer]:\" in chain_answer:\n",
    "        answer = chain_answer.split(\"[Final Answer]:\")[-1].strip()\n",
    "        return answer\n",
    "    return None"
   ],
   "id": "25291f1e31a6f77d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5e6e776033e4247818ccf8a76716d51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ],
   "id": "813e47789013683b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-18T12:46:53.166672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Retrieve top-k contexts and evaluate with Exact Match\n",
    "def evaluate_with_top_k(k_values: List[int]) -> Dict[str, Dict[str, float]]:\n",
    "    response = con.retrieve(corpus, queries, top_k=max(k_values), score_function=similarity_measure)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response\n",
    "\n",
    "response: Dict[str, Dict[str, float]] = evaluate_with_top_k(k_values)"
   ],
   "id": "964620ce640dc3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb torch.Size([1200, 35, 768])\n",
      "sentence_emb torch.Size([1200, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/563424 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding of contexts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 62240/563424 [29:37<3:58:42, 34.99it/s] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ],
   "id": "d2e8ff89cee63d0b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "99138e5d402cc642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ],
   "id": "62297b9db0795fd"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_random_contexts(relevant_docs_by_query, relevant_ratio=0.5):\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for query in queries:\n",
    "        query_id = query.id()\n",
    "        top_k_relevant_docs_ids = list(relevant_docs_by_query[query_id].keys())\n",
    "        top_k_relevant_docs = [evidence for evidence in corpus if evidence.id in top_k_relevant_docs_ids]\n",
    "        \n",
    "        # Get non relevant docs from qrels (where qrels[query_id][doc_id] = 0)\n",
    "        non_relevant_doc_ids = {doc_id for doc_id, relevance in qrels.get(query_id, {}).items() if relevance == 0}\n",
    "        random_non_relevant_docs = [evidence for evidence in corpus if evidence.id in non_relevant_doc_ids]\n",
    "        num_random_docs = int(len(top_k_relevant_docs) * (1 - relevant_ratio) / relevant_ratio)\n",
    "        # TODO think on how to not get duplicates\n",
    "        random_docs = random_non_relevant_docs[:num_random_docs]\n",
    "        # random_docs = random.sample(random_non_relevant_docs, min(num_random_docs, len(non_relevant_doc_ids)))\n",
    "        \n",
    "        # Combine relevant and random non-relevant docs\n",
    "        combined_docs = top_k_relevant_docs + random_docs\n",
    "        random.shuffle(combined_docs)\n",
    "        \n",
    "        # Pass to LLM for evaluation\n",
    "        answer = query_llm(query.text(), combined_docs)\n",
    "        ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "        \n",
    "        # Evaluate the answer\n",
    "        if answer is not None and ground_truth_answer.lower() in answer.lower():\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "        \n",
    "        results[query_id] = answer\n",
    "        if i == 5:\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "evaluate_with_random_contexts(response)"
   ],
   "id": "25639be0d7fa46ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ],
   "id": "4e75723788b2bd77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:10.266634Z",
     "start_time": "2024-12-13T10:36:10.249692Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "90f91ef62c7a9019",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ],
   "id": "bb93e8c6a3e03131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:13.851897Z",
     "start_time": "2024-12-13T10:36:13.847910Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b87a7ff1f833c9a7",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cebe5da82911b029"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
