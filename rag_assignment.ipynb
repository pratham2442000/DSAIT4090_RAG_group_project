{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd8a92acdf73e00",
   "metadata": {},
   "source": [
    "## NLP Assignment: RAGs For Open Domain Complex QA\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Henry: stuff for training on colab, do not delete pls :D",
   "id": "86406812b1c05c91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Google colab does not save your files and will delete them when your session terminates, persist models in google drive.\n",
    "# from google.colab import drive\n",
    "# import shutil\n",
    "# drive.mount('/content/drive')"
   ],
   "id": "dd4dd326242676e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T19:09:06.154689Z",
     "start_time": "2025-01-04T19:09:06.152163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Google Colab Execution Steps\n",
    "# STEPS TO RUN ON GOOGLE COLAB\n",
    "# 1.) Select the ipynb file as your notebook\n",
    "# 2.) Change your runtime to the free tier GPU\n",
    "# upload a zip file of this project called NLPProject.zip under '/content' which is the working directory of the notebook\n",
    "# 3.) run this script\n",
    "# !rm -R sample_data/\n",
    "# !unzip NLPProject.zip\n",
    "# !rm -rf NLPProject/rag_assignment.ipynb\n",
    "# !mv NLPProject/* .\n",
    "# !rm -R NLPProject\n",
    "# !rm -rf NLPProject.zip\n",
    "# !pip install -e .\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/index.joblib','/content/indices/adore/corpus')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/dev.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/test.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/train.json','/content/data')\n",
    "# shutil.copy('/content/drive/MyDrive/ColabFiles/wiki_musique_corpus.json','/content/data')\n"
   ],
   "id": "fb0d85e683cc4ade",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a07a59e9fd150a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T01:57:23.309011Z",
     "start_time": "2025-01-08T01:57:21.331399Z"
    }
   },
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.ADORERetriever import ADORERetriever\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore, DotScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import csv"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/anaconda3/envs/rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a50e93acf28a226e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T01:57:23.760623Z",
     "start_time": "2025-01-08T01:57:23.757095Z"
    }
   },
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "55423df49501736c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T01:57:25.709585Z",
     "start_time": "2025-01-08T01:57:25.706477Z"
    }
   },
   "source": [
    "# -----------------------------------------\n",
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "#\n",
    "# Contriever taken from huggingface\n",
    "# model_path = 'huggingface/contriever'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "# model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Llama taken from huggingface\n",
    "# model_name = \"huggingface/llama\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# But uncomment these two\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_path = \"facebook/contriever\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8719d6cb521f8539",
   "metadata": {},
   "source": "### Load the Dataset and set up contriever\n"
  },
  {
   "cell_type": "code",
   "id": "3a7c486097b42d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:02:22.762667Z",
     "start_time": "2025-01-08T01:57:27.139529Z"
    }
   },
   "source": [
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1 if relevant\n",
    "corpus: List[Evidence]\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "# Load dataset with dev set\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "# Extract ground truth answers for the questions\n",
    "raw_data = loader.base_dataset.raw_data\n",
    "question_ground_truth_answer_map = {sample.question.id(): sample.answer.text() for sample in raw_data}\n",
    "\n",
    "# Setup contriever\n",
    "con = Contriever(config_instance)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1014284.60it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 380648.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:46<00:00,  4.18it/s]\n",
      "/home/henry/anaconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/henry/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/henry/anaconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LLM",
   "id": "24917b889cc8071e"
  },
  {
   "cell_type": "code",
   "id": "25291f1e31a6f77d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T07:40:00.866558Z",
     "start_time": "2024-12-26T07:39:56.263579Z"
    }
   },
   "source": [
    "from dexter.llms.llama_engine import LlamaEngine\n",
    "# Setup LLM\n",
    "llm_instance = LlamaEngine(data=\"\", model_name=model_name, temperature=0.3, top_n=1)\n",
    "\n",
    "# Code to query llm\n",
    "def query_llm(question_text: str, evidences: List[Evidence]):\n",
    "    \"\"\"\n",
    "    :param question_text: question text\n",
    "    :param evidences: list of evidences\n",
    "    :return: the answer or None if no answer\n",
    "    \"\"\"\n",
    "    evidence_text = \"\\n\".join(doc.text() for doc in evidences)\n",
    "    system_prompt = \"Follow the given examples and Given the question and context output final answer for the question using information in the context and give answer in form of  [Final Answer]: \\n\"\n",
    "    user_prompt = f\"Question: {question_text}\\nContext: {evidence_text}\\nAnswer:\"\n",
    "\n",
    "    chain_answer = llm_instance.get_llama_completion(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the response\n",
    "    if \"not possible\" in chain_answer.lower() or \"unknown\" in chain_answer.lower():\n",
    "        return None\n",
    "    elif \"[Final Answer]:\" in chain_answer:\n",
    "        answer = chain_answer.split(\"[Final Answer]:\")[-1].strip()\n",
    "        return answer\n",
    "        \n",
    "    return None"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3624a3c27c9842aba005c5f4a497a9de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30c968dc12214fc7a83e4649531b54fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieve all \"relevant\" docs based on similarity",
   "id": "2cfccb7bcad45819"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:04:53.012899Z",
     "start_time": "2025-01-06T17:04:52.997164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_from_retrieved(retrieved: Dict[str, Dict[str, float]], top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract the top-k documents for each query from the retrieved results.\n",
    "    This is to avoid calling con.retrieve multiple times.\n",
    "    \"\"\"\n",
    "    top_k_results = {}\n",
    "    for query_id, docs_scores in retrieved.items():\n",
    "        sorted_docs = sorted(docs_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_results[query_id] = {doc_id: score for doc_id, score in sorted_docs[:top_k]}\n",
    "    return top_k_results"
   ],
   "id": "2c0d1181ea548c02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:24:43.452960Z",
     "start_time": "2025-01-06T17:04:53.013897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "\n",
    "# Calculate similarities for all queries and docs\n",
    "similar_docs_by_query = con.retrieve(corpus, queries, top_k=100, score_function=similarity_measure, chunk=True, chunksize=400000)"
   ],
   "id": "171d35299443e5ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [14:04<00:00, 473.56it/s]\n",
      "100%|██████████| 163424/163424 [05:44<00:00, 474.30it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "813e47789013683b",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:24:43.499969Z",
     "start_time": "2025-01-06T17:24:43.453966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Evaluate top_k contexts with Exact Match, use previously retrieved results\n",
    "def evaluate_with_top_k(retrieved, top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    response = get_top_k_from_retrieved(retrieved, top_k)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response\n",
    "\n",
    "k = max(k_values)\n",
    "top_k_docs_by_query: Dict[str, Dict[str, float]] = evaluate_with_top_k(similar_docs_by_query, k)"
   ],
   "id": "567a36ecc4fc5fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices retrieved: 1200\n",
      "({'NDCG@1': 0.42417, 'NDCG@2': 0.3858, 'NDCG@3': 0.34182}, {'MAP@1': 0.04266, 'MAP@2': 0.0646, 'MAP@3': 0.0762}, {'Recall@1': 0.04266, 'Recall@2': 0.07536, 'Recall@3': 0.09527}, {'P@1': 0.42417, 'P@2': 0.37458, 'P@3': 0.31583})\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d2e8ff89cee63d0b",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ]
  },
  {
   "cell_type": "code",
   "id": "99138e5d402cc642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T08:00:01.534483Z",
     "start_time": "2024-12-26T08:00:01.519069Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "62297b9db0795fd",
   "metadata": {},
   "source": [
    "### Task 3 & 4\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T08:00:01.549735Z",
     "start_time": "2024-12-26T08:00:01.534783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query):\n",
    "    \"\"\"\n",
    "    Evaluates a single query by selecting relevant and non-relevant documents,\n",
    "    combining them, passing them to the LLM, and comparing the answer to the ground truth.\n",
    "    \n",
    "    :param query: The query to be evaluated.\n",
    "    :param non_relevant_doc_ids_of_query: The ids of the non-relevant documents.\n",
    "    :param top_k_similar_doc_ids_of_query: The ids of the similar documents.\n",
    "    :return: A boolean ismatch indicating whether the evaluation is correct.\n",
    "    \"\"\"\n",
    "    # Fetch the actual similar documents (not just ids)\n",
    "    top_k_similar_docs_of_query = [doc for doc in corpus if doc.id() in top_k_similar_doc_ids_of_query]\n",
    "    \n",
    "    # Convert selected doc IDs to actual documents\n",
    "    non_relevant_docs = [doc for doc in corpus if doc.id() in non_relevant_doc_ids_of_query]\n",
    "    \n",
    "    # Combine relevant and selected non-relevant docs\n",
    "    combined_docs = top_k_similar_docs_of_query + non_relevant_docs\n",
    "    random.shuffle(combined_docs)\n",
    "    \n",
    "    # Evaluate the LLM answer\n",
    "    answer = query_llm(query.text(), combined_docs)\n",
    "    ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "    \n",
    "    # Check if the answer matches the ground truth\n",
    "    is_match = answer is not None and ground_truth_answer.lower() in answer.lower()\n",
    "\n",
    "    # Write the result to the CSV file\n",
    "    relevant_text = \"\\n\".join(doc.text() for doc in top_k_similar_docs_of_query)\n",
    "    non_relevant_text = \"\\n\".join(doc.text() for doc in non_relevant_docs)\n",
    "    row = [query.text(), ground_truth_answer, answer, is_match, relevant_text, non_relevant_text]\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # print(\"Answer: \", answer)\n",
    "    # print(\"Ground truth answer: \", ground_truth_answer)\n",
    "    # print(\"Is match: \", is_match)\n",
    "    \n",
    "    return is_match"
   ],
   "id": "ec3f5e86e0841065",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T08:00:01.565753Z",
     "start_time": "2024-12-26T08:00:01.550732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_all_non_relevant(num_docs_to_sample, is_hard_negatives):\n",
    "    \"\"\"\n",
    "    Computes all non-relevant docs (aka random or hard negatives) for a list of queries and a corpus.\n",
    "    \"\"\"\n",
    "    all_non_relevant = {}\n",
    "\n",
    "    for query in tqdm(queries, desc=f\"Retrieving {num_docs_to_sample} non-relevant docs for each query\", unit=\"query\"):\n",
    "        query_id = query.id()\n",
    "\n",
    "        # Extract non-relevant document IDs based on qrels for this query\n",
    "        non_relevant_doc_ids = {doc.id() for doc in corpus if not (doc.id() in qrels[query_id])}\n",
    "        \n",
    "        if is_hard_negatives:\n",
    "            most_similar_docs = get_top_k_from_retrieved(similar_docs_by_query, 100)\n",
    "            # Get similar and nonrelevant docs, remove the ones with similarity 0\n",
    "            similar_and_non_relevant_doc_ids = {doc_id for doc_id in most_similar_docs[query_id] if most_similar_docs[query_id][doc_id] > 0 and doc_id in non_relevant_doc_ids}\n",
    "            non_relevant_doc_ids = similar_and_non_relevant_doc_ids\n",
    "\n",
    "        if num_docs_to_sample > len(non_relevant_doc_ids):\n",
    "            \n",
    "            print(\"Not enough documents to sample from, please select smaller ratio or reduce the number of similar docs per query to select\")\n",
    "            return None\n",
    "        all_non_relevant[query_id] = list(non_relevant_doc_ids)[:num_docs_to_sample]\n",
    "\n",
    "    return all_non_relevant\n"
   ],
   "id": "687cb964bb805e0e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T08:00:01.580218Z",
     "start_time": "2024-12-26T08:00:01.565753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_docs(top_k, non_relevant_doc_ids_by_query, num_docs_to_sample):\n",
    "    \"\"\"\n",
    "    General evaluation function for both random and hard negatives.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    print(f\"There are {top_k} relevant docs and {num_docs_to_sample} non-relative docs per query\")\n",
    "    \n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['Question', 'Ground Truth Answer', 'LLM Answer', 'Match', 'Relevant Documents', 'Non-Relevant Documents']\n",
    "        writer.writerow(header)\n",
    "    \n",
    "    for query in tqdm(queries, desc=\"Evaluating queries\", unit=\"query\"):\n",
    "        top_k_similar_doc_ids_of_query = set(top_k_similar_docs_by_query[query.id()].keys())\n",
    "        non_relevant_doc_ids_of_query = non_relevant_doc_ids_by_query[query.id()]\n",
    "        is_match = evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query)\n",
    "        if is_match:\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "    return matches, mismatches"
   ],
   "id": "28b5e803d0114d8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:32:57.543185Z",
     "start_time": "2024-12-26T21:55:17.065678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try with only relevant contexts\n",
    "samples_values = [12, 24]\n",
    "for top_k in samples_values:\n",
    "    output_file = f'llm_relevant_results-{top_k}-0.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_other_docs_to_sample=0\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    other_doc_ids_by_query = compute_all_non_relevant(0, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, other_doc_ids_by_query, num_docs_to_sample=0)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "dd1b8c4e337cded8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=12, num_other_docs_to_sample=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 0 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:22<00:00,  8.44query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 relevant docs and 0 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [44:15<00:00,  2.21s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 686\n",
      "Mismatches: 514\n",
      "Running for top_k=24, num_other_docs_to_sample=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 0 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:19<00:00,  8.58query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 relevant docs and 0 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [48:42<00:00,  2.44s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 703\n",
      "Mismatches: 497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 3",
   "id": "ea769656e45e19af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T01:44:24.128736Z",
     "start_time": "2024-12-26T23:32:57.544184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try with only random contexts\n",
    "samples_values = [12, 24]\n",
    "top_k = 0\n",
    "for num_random_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "c42edf3641bfa318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=0, num_random_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:12<00:00,  9.03query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [1:13:46<00:00,  3.69s/query]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 539\n",
      "Mismatches: 661\n",
      "Running for top_k=0, num_random_docs_to_sample=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 24 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:13<00:00,  8.97query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 relevant docs and 24 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [53:13<00:00,  2.66s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 563\n",
      "Mismatches: 637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T10:42:48.496246Z",
     "start_time": "2024-12-26T08:00:01.580419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pairs of total 12: 3+9, 6+6, 9+3\n",
    "top_k_values = [3, 6, 9]\n",
    "samples_values = [9, 6, 3]\n",
    "# Evaluate with random context\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "a2e7eb386ee84b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=3, num_random_docs_to_sample=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 9 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:23<00:00,  8.35query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 relevant docs and 9 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   1%|          | 10/1200 [00:40<1:25:37,  4.32s/query]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating queries: 100%|██████████| 1200/1200 [59:21<00:00,  2.97s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 605\n",
      "Mismatches: 595\n",
      "Running for top_k=6, num_random_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:19<00:00,  8.61query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [50:46<00:00,  2.54s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 658\n",
      "Mismatches: 542\n",
      "Running for top_k=9, num_random_docs_to_sample=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 3 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:13<00:00,  8.99query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 relevant docs and 3 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [45:42<00:00,  2.29s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 678\n",
      "Mismatches: 522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:19:23.123532Z",
     "start_time": "2024-12-26T10:42:48.497785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pairs of total 24: 6+18, 12+12, 18+6\n",
    "top_k_values = [6, 12, 18]\n",
    "samples_values = [18, 12, 6]\n",
    "# Evaluate with random context\n",
    "for top_k, num_random_docs_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_random_docs_to_sample={num_random_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "3164f4b543116c24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=6, num_random_docs_to_sample=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 18 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:18<00:00,  8.66query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 18 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   4%|▍         | 54/1200 [02:08<30:42,  1.61s/query]  This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Evaluating queries: 100%|██████████| 1200/1200 [51:11<00:00,  2.56s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 671\n",
      "Mismatches: 529\n",
      "Running for top_k=12, num_random_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:24<00:00,  8.28query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [49:45<00:00,  2.49s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 714\n",
      "Mismatches: 486\n",
      "Running for top_k=18, num_random_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:18<00:00,  8.66query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [48:35<00:00,  2.43s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 714\n",
      "Mismatches: 486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ],
   "id": "80d6f5137f5a5828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:59:50.050820Z",
     "start_time": "2024-12-26T18:08:20.844261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add only hard negatives\n",
    "# Try with only contexts\n",
    "samples_values = [12, 24]\n",
    "top_k = 0\n",
    "for num_hard_neg_docs_to_sample in samples_values:\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_neg_docs_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_neg_docs_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_neg_docs_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_neg_docs_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "79ddd989d9468ff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=0, num_hard_neg_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:36<00:00,  7.69query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [50:57<00:00,  2.55s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 555\n",
      "Mismatches: 645\n",
      "Running for top_k=0, num_hard_neg_docs_to_sample=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 24 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:37<00:00,  7.63query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 relevant docs and 24 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [55:18<00:00,  2.77s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 579\n",
      "Mismatches: 621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T15:40:07.364048Z",
     "start_time": "2024-12-26T13:19:23.123716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hard negatives are the documents both in retrieved docs (they are similar) and non-relevant docs (since they are not actually relevant)\n",
    "# Evaluate with hard negative contexts\n",
    "# Pairs of total 12: 3+9, 6+6, 9+3\n",
    "top_k_values = [3, 6, 9]\n",
    "samples_values = [9, 6, 3]\n",
    "\n",
    "for top_k, num_hard_negatives_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_negatives_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_negatives_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "582d7aa98394eeb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=3, num_hard_neg_docs_to_sample=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 9 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:33<00:00,  7.80query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 relevant docs and 9 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [44:55<00:00,  2.25s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 620\n",
      "Mismatches: 580\n",
      "Running for top_k=6, num_hard_neg_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:45<00:00,  7.27query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [44:11<00:00,  2.21s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 654\n",
      "Mismatches: 546\n",
      "Running for top_k=9, num_hard_neg_docs_to_sample=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 3 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:41<00:00,  7.43query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 relevant docs and 3 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [43:36<00:00,  2.18s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 673\n",
      "Mismatches: 527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T18:08:20.844142Z",
     "start_time": "2024-12-26T15:40:07.365076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pairs of total 24: 6+18, 12+12, 18+6\n",
    "top_k_values = [6, 12, 18]\n",
    "samples_values = [18, 12, 6]\n",
    "for top_k, num_hard_negatives_to_sample in zip(top_k_values, samples_values):\n",
    "    output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "    print(f\"Running for top_k={top_k}, num_hard_neg_docs_to_sample={num_hard_negatives_to_sample}\")\n",
    "    \n",
    "    top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)\n",
    "    hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "    matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_negatives_to_sample)\n",
    "    \n",
    "    print(\"Matches:\", matches)\n",
    "    print(\"Mismatches:\", mismatches)"
   ],
   "id": "7dbbb70f2ad541ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for top_k=6, num_hard_neg_docs_to_sample=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 18 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:35<00:00,  7.70query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 relevant docs and 18 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [47:12<00:00,  2.36s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 636\n",
      "Mismatches: 564\n",
      "Running for top_k=12, num_hard_neg_docs_to_sample=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 12 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:36<00:00,  7.67query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 relevant docs and 12 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [47:21<00:00,  2.37s/query] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 674\n",
      "Mismatches: 526\n",
      "Running for top_k=18, num_hard_neg_docs_to_sample=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 6 non-relevant docs for each query: 100%|██████████| 1200/1200 [02:36<00:00,  7.67query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 relevant docs and 6 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1200/1200 [45:51<00:00,  2.29s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 689\n",
      "Mismatches: 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "bb93e8c6a3e03131",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ]
  },
  {
   "cell_type": "code",
   "id": "b87a7ff1f833c9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:02:58.754618Z",
     "start_time": "2025-01-08T02:02:56.411966Z"
    }
   },
   "source": [
    "adore_config = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32\n",
    "                                    ,learning_rate=5e-6)\n",
    "# Setup contriever\n",
    "adore_retriever = ADORERetriever(adore_config)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['314523', '387778', '550339', '69975', '126271', '35800', '369523', '401226', '237230', '209617'])\n",
      "dict_keys(['546048', '404896', '458648', '250316', '40416', '99061', '143517', '280200', '380442', '70651'])\n",
      "dict_keys(['271949', '175303', '79089', '193912', '214158', '81390', '273667', '9415', '438703', '15376'])\n",
      "dict_keys(['227835', '322620', '402608', '73488', '446247', '103924', '171055', '354608', '238514', '107754'])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "cebe5da82911b029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T00:59:02.724656Z",
     "start_time": "2025-01-08T00:58:27.405759Z"
    }
   },
   "source": "adore_retriever.train(queries=queries, corpus=corpus, qrels=qrels, top_k=50, n_epochs=10)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING PASSAGE EMBEDDINGS FOR ADORE TRAINING - THIS MIGHT TAKE A WHILE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1024it [00:34, 29.96it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE BUILDING CORPUS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch progress:   0%|          | 0/1900 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43madore_retriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqrels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqrels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/school/NLPProject/dexter/retriever/dense/ADORERetriever.py:215\u001B[0m, in \u001B[0;36mADORERetriever.train\u001B[0;34m(self, queries, corpus, qrels, top_k, n_epochs)\u001B[0m\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;66;03m# we take the top_k and eliminate the +20 part, this will only have hard-negatives\u001B[39;00m\n\u001B[1;32m    213\u001B[0m     all_hard_negative_idxs \u001B[38;5;241m=\u001B[39m all_hard_negative_idxs[:top_k]\n\u001B[0;32m--> 215\u001B[0m     batch_total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mcompute_loss_for_query_and_hard_negatives\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelevant_doc_idxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqrels_tensor_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mall_hard_negative_idxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_hard_negative_idxs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43msimilarity_scores_1d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimilarity_scores\u001B[49m\u001B[43m[\u001B[49m\u001B[43mq_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m loss \u001B[38;5;241m=\u001B[39m batch_total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(cur_queries)\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n",
      "File \u001B[0;32m~/school/NLPProject/dexter/retriever/dense/ADORERetriever.py:67\u001B[0m, in \u001B[0;36mcompute_loss_for_query_and_hard_negatives\u001B[0;34m(relevant_doc_idxs, all_hard_negative_idxs, similarity_scores_1d)\u001B[0m\n\u001B[1;32m     64\u001B[0m hard_negative_scores \u001B[38;5;241m=\u001B[39m similarity_scores_1d[all_hard_negative_idxs]\n\u001B[1;32m     66\u001B[0m all_scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([relevant_scores, hard_negative_scores])\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mall_scores\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m all_doc_idxs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([relevant_doc_idxs, all_hard_negative_idxs])\n\u001B[1;32m     69\u001B[0m sorted_temp \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margsort(all_scores, descending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/rag/lib/python3.10/site-packages/torch/_tensor.py:523\u001B[0m, in \u001B[0;36mTensor.__repr__\u001B[0;34m(self, tensor_contents)\u001B[0m\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    520\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__repr__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, tensor_contents\u001B[38;5;241m=\u001B[39mtensor_contents\n\u001B[1;32m    521\u001B[0m     )\n\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# All strings are unicode in Python 3.\u001B[39;00m\n\u001B[0;32m--> 523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tensor_str\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rag/lib/python3.10/site-packages/torch/_tensor_str.py:708\u001B[0m, in \u001B[0;36m_str\u001B[0;34m(self, tensor_contents)\u001B[0m\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(), torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39m_python_dispatch\u001B[38;5;241m.\u001B[39m_disable_current_modes():\n\u001B[1;32m    707\u001B[0m     guard \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_DisableFuncTorch()\n\u001B[0;32m--> 708\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_str_intern\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rag/lib/python3.10/site-packages/torch/_tensor_str.py:625\u001B[0m, in \u001B[0;36m_str_intern\u001B[0;34m(inp, tensor_contents)\u001B[0m\n\u001B[1;32m    623\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m _tensor_str(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dense(), indent)\n\u001B[1;32m    624\u001B[0m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 625\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m \u001B[43m_tensor_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstrided:\n\u001B[1;32m    628\u001B[0m     suffixes\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout))\n",
      "File \u001B[0;32m~/anaconda3/envs/rag/lib/python3.10/site-packages/torch/_tensor_str.py:357\u001B[0m, in \u001B[0;36m_tensor_str\u001B[0;34m(self, indent)\u001B[0m\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\n\u001B[1;32m    354\u001B[0m         \u001B[38;5;28mself\u001B[39m, indent, summarize, real_formatter, imag_formatter\n\u001B[1;32m    355\u001B[0m     )\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 357\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m \u001B[43m_Formatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_summarized_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\u001B[38;5;28mself\u001B[39m, indent, summarize, formatter)\n",
      "File \u001B[0;32m~/anaconda3/envs/rag/lib/python3.10/site-packages/torch/_tensor_str.py:146\u001B[0m, in \u001B[0;36m_Formatter.__init__\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mlen\u001B[39m(value_str))\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    145\u001B[0m     nonzero_finite_vals \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmasked_select(\n\u001B[0;32m--> 146\u001B[0m         tensor_view, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misfinite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor_view\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m&\u001B[39m tensor_view\u001B[38;5;241m.\u001B[39mne(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    147\u001B[0m     )\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nonzero_finite_vals\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    150\u001B[0m         \u001B[38;5;66;03m# no valid number, do nothing\u001B[39;00m\n\u001B[1;32m    151\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:21:05.236597Z",
     "start_time": "2025-01-07T05:21:04.956929Z"
    }
   },
   "cell_type": "code",
   "source": "adore_retriever.save_query_encoder(\"results\")",
   "id": "c174be69b314d8a0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:46:22.885181Z",
     "start_time": "2025-01-07T12:26:07.761503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# THIS MUST BE THE SAME AS THE SIMILARITY METRIC BEING USED IN ADORERETRIEVER.PY\n",
    "similarity_measure = CosScore()\n",
    "adore_retriever.load_query_encoder(\"results\")\n",
    "\n",
    "con.question_encoder = adore_retriever.question_encoder\n",
    "con.question_encoder.to('cuda')\n",
    "similar_docs_by_query_adore = con.retrieve(corpus, queries, top_k=100, score_function=similarity_measure, chunk=True, chunksize=400000)"
   ],
   "id": "3e189b5e41841b45",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [14:20<00:00, 464.62it/s]\n",
      "100%|██████████| 163424/163424 [05:53<00:00, 462.89it/s]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:46:35.176124Z",
     "start_time": "2025-01-07T12:46:35.145717Z"
    }
   },
   "cell_type": "code",
   "source": "top_k_docs_by_query_adore = evaluate_with_top_k(similar_docs_by_query_adore, k)",
   "id": "4c5e695484d72ee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices retrieved: 1200\n",
      "({'NDCG@1': 0.40667, 'NDCG@2': 0.34219, 'NDCG@3': 0.30257}, {'MAP@1': 0.04081, 'MAP@2': 0.05512, 'MAP@3': 0.06426}, {'Recall@1': 0.04081, 'Recall@2': 0.06492, 'Recall@3': 0.08231}, {'P@1': 0.40667, 'P@2': 0.32333, 'P@3': 0.27333})\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "abb7d8edad975b41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
