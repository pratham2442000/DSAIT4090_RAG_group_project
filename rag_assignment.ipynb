{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd8a92acdf73e00",
   "metadata": {},
   "source": [
    "## NLP Assignment: RAGs For Open Domain Complex QA\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a07a59e9fd150a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:02:21.631969Z",
     "start_time": "2024-12-21T19:02:18.794098Z"
    }
   },
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from dexter.data.datastructures.question import Question\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find CUDA.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a50e93acf28a226e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:02:21.695003Z",
     "start_time": "2024-12-21T19:02:21.632966Z"
    }
   },
   "source": [
    "# Check torch version and make sure cuda is enabled and available\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "55423df49501736c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:02:22.160488Z",
     "start_time": "2024-12-21T19:02:21.695003Z"
    }
   },
   "source": [
    "# -----------------------------------------\n",
    "# Since china doesn't have access to huggingface, I have manually downloaded the model, feel free to comment this.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Contriever taken from huggingface\n",
    "model_path = 'huggingface/contriever'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Llama taken from huggingface\n",
    "model_name = \"huggingface/llama\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# But uncomment these two\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_path = \"facebook/contriever\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8719d6cb521f8539",
   "metadata": {},
   "source": "### Load the Dataset and set up contriever\n"
  },
  {
   "cell_type": "code",
   "id": "3a7c486097b42d53",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-21T19:02:22.161442Z"
    }
   },
   "source": [
    "from dexter.llms.llama_engine import LlamaEngine\n",
    "\n",
    "queries: List[Question]\n",
    "qrels: Dict[str, Dict[str, int]] # qrels[question id][evidence id] = 1/0 true false if relevant\n",
    "corpus: List[Evidence]\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=model_path,\n",
    "                                    document_encoder_path=model_path\n",
    "                                    ,batch_size=32)\n",
    "\n",
    "# Load dataset with dev set\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wiki_musique_corpus\",\"config.ini\",Split.DEV,tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "# Extract ground truth answers for the questions\n",
    "raw_data = loader.base_dataset.raw_data\n",
    "question_ground_truth_answer_map = {sample.question.id(): sample.answer.text() for sample in raw_data}\n",
    "\n",
    "# Setup contriever\n",
    "con = Contriever(config_instance)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1771423.86it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:00<00:00, 638999.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 326/1200 [00:24<01:10, 12.42it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LLM",
   "id": "24917b889cc8071e"
  },
  {
   "cell_type": "code",
   "id": "25291f1e31a6f77d",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Setup LLM\n",
    "llm_instance = LlamaEngine(data=\"\", model_name=model_name, temperature=0.3, top_n=1)\n",
    "\n",
    "# Code to query llm\n",
    "def query_llm(question_text: str, evidences: List[Evidence]):\n",
    "    \"\"\"\n",
    "    :param question_text: question text\n",
    "    :param evidences: list of evidences\n",
    "    :return: the answer or None if no answer\n",
    "    \"\"\"\n",
    "    evidence_text = \"\\n\".join(doc.text() for doc in evidences)\n",
    "    system_prompt = \"Follow the given examples and Given the question and context output final answer for the question using information in the context and give answer in form of  [Final Answer]: \\n\"\n",
    "    user_prompt = f\"Question: {question_text}\\nContext: {evidence_text}\\nAnswer:\"\n",
    "\n",
    "    chain_answer = llm_instance.get_llama_completion(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the response\n",
    "    if \"not possible\" in chain_answer.lower() or \"unknown\" in chain_answer.lower():\n",
    "        return None\n",
    "    elif \"[Final Answer]:\" in chain_answer:\n",
    "        answer = chain_answer.split(\"[Final Answer]:\")[-1].strip()\n",
    "        return answer\n",
    "        \n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieve all \"relevant\" docs based on similarity",
   "id": "2cfccb7bcad45819"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_from_retrieved(retrieved: Dict[str, Dict[str, float]], top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract the top-k documents for each query from the retrieved results.\n",
    "    This is to avoid calling con.retrieve multiple times.\n",
    "    \"\"\"\n",
    "    top_k_results = {}\n",
    "    for query_id, docs_scores in retrieved.items():\n",
    "        sorted_docs = sorted(docs_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_results[query_id] = {doc_id: score for doc_id, score in sorted_docs[:top_k]}\n",
    "    return top_k_results"
   ],
   "id": "2c0d1181ea548c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_measure = CosScore()\n",
    "\n",
    "# Calculate similarities for all queries and docs\n",
    "similar_docs_by_query = con.retrieve(corpus, queries, top_k=100, score_function=similarity_measure, chunk=True, chunksize=400000)"
   ],
   "id": "171d35299443e5ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "813e47789013683b",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Use off the shelf retriever (contriever) and extract contexts for each query to be given as input to a generative model. Use Exact Match or cover Exact Match as metric for evaluating generated answers. Experiment with k=1,3,5 for retrieving top-k contexts and report the performance on generating answers."
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "k_values = [1,2,3]\n",
    "metrics = RetrievalMetrics(k_values=k_values)\n",
    "\n",
    "# Evaluate top_k contexts with Exact Match, use previously retrieved results\n",
    "def evaluate_with_top_k(retrieved, top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    response = get_top_k_from_retrieved(retrieved, top_k)\n",
    "    print(\"Indices retrieved:\", len(response))\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels, results=response))\n",
    "    return response\n",
    "\n",
    "k = max(k_values)\n",
    "top_k_docs_by_query: Dict[str, Dict[str, float]] = evaluate_with_top_k(similar_docs_by_query, k)"
   ],
   "id": "567a36ecc4fc5fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2e8ff89cee63d0b",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Repeat the above experiment without the retriever, using only oracle contexts as input. Oracle\n",
    "contexts are annotated documents provided for each question in dev.json."
   ]
  },
  {
   "cell_type": "code",
   "id": "99138e5d402cc642",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62297b9db0795fd",
   "metadata": {},
   "source": [
    "### Task 3 & 4\n",
    "Now randomly sample documents from the collection that are not relevant to the current query during inference on the evaluation set. Combine these documents with the top-k relevant documents and use them as input to the LLM for answering a query. You can decide the ratios to mix the relevant and the random documents that serve as noise. Analyze the performance."
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query):\n",
    "    \"\"\"\n",
    "    Evaluates a single query by selecting relevant and non-relevant documents,\n",
    "    combining them, passing them to the LLM, and comparing the answer to the ground truth.\n",
    "    \n",
    "    :param query: The query to be evaluated.\n",
    "    :param non_relevant_doc_ids_of_query: The ids of the non-relevant documents.\n",
    "    :param top_k_similar_doc_ids_of_query: The ids of the similar documents.\n",
    "    :return: A boolean ismatch indicating whether the evaluation is correct.\n",
    "    \"\"\"\n",
    "    if len(top_k_similar_doc_ids_of_query) == 0:\n",
    "        print(f\"No relevant documents were selected for this query {query.id()}\")\n",
    "        return False\n",
    "    \n",
    "    # Fetch the actual similar documents (not just ids)\n",
    "    top_k_similar_docs_of_query = [doc for doc in corpus if doc.id() in top_k_similar_doc_ids_of_query]\n",
    "    \n",
    "    # Convert selected doc IDs to actual documents\n",
    "    non_relevant_docs = [doc for doc in corpus if doc.id() in non_relevant_doc_ids_of_query]\n",
    "    \n",
    "    # Combine relevant and selected non-relevant docs\n",
    "    combined_docs = top_k_similar_docs_of_query + non_relevant_docs\n",
    "    random.shuffle(combined_docs)\n",
    "    \n",
    "    # Evaluate the LLM answer\n",
    "    answer = query_llm(query.text(), combined_docs)\n",
    "    ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "    \n",
    "    # Check if the answer matches the ground truth\n",
    "    is_match = answer is not None and ground_truth_answer.lower() in answer.lower()\n",
    "\n",
    "    # Write the result to the CSV file\n",
    "    relevant_text = \"\\n\".join(doc.text() for doc in top_k_similar_docs_of_query)\n",
    "    non_relevant_text = \"\\n\".join(doc.text() for doc in non_relevant_docs)\n",
    "    row = [query.text(), ground_truth_answer, answer, is_match, relevant_text, non_relevant_text]\n",
    "    with open(\"llm_results.csv\", mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # print(\"Answer: \", answer)\n",
    "    # print(\"Ground truth answer: \", ground_truth_answer)\n",
    "    # print(\"Is match: \", is_match)\n",
    "    \n",
    "    return is_match"
   ],
   "id": "ec3f5e86e0841065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_all_non_relevant(num_docs_to_sample, is_hard_negatives):\n",
    "    \"\"\"\n",
    "    Computes all hard negatives for a list of queries and a corpus using cosine similarity.\n",
    "    \"\"\"\n",
    "    all_non_relevant = {}\n",
    "\n",
    "    for query in tqdm(queries, desc=\"Retrieving non-relevant docs for each query\", unit=\"query\"):\n",
    "        query_id = query.id()\n",
    "\n",
    "        # Extract non-relevant document IDs based on qrels for this query\n",
    "        non_relevant_doc_ids = {doc.id() for doc in corpus if not (doc.id() in qrels[query_id])}\n",
    "        \n",
    "        if is_hard_negatives:\n",
    "            sampled_non_relevant_ids = random.sample(non_relevant_doc_ids, min(len(non_relevant_doc_ids), num_docs_to_sample*5))  # This is a bit random\n",
    "            sampled_docs = [doc for doc in corpus if doc.id() in sampled_non_relevant_ids]\n",
    "\n",
    "            query_embedding = con.encode_queries([query], batch_size=con.batch_size)  # Shape: [1, D] - encode query\n",
    "            non_relevant_embeddings = con.encode_corpus(sampled_docs)\n",
    "\n",
    "            # Compute cosine similarity for the sampled documents\n",
    "            cos_scores = similarity_measure.evaluate(query_embedding, non_relevant_embeddings)  # Shape: [1, sampled_size*5]\n",
    "\n",
    "            # Select documents with similarity > 0 (you can adjust this threshold)\n",
    "            hard_negatives = [\n",
    "                doc.id() for doc, score in zip(sampled_docs, cos_scores[0].tolist())\n",
    "                if score > 0\n",
    "            ]\n",
    "\n",
    "            # Update non-relevant docs list with hard negatives\n",
    "            non_relevant_doc_ids = hard_negatives\n",
    "\n",
    "        if num_docs_to_sample > len(non_relevant_doc_ids):\n",
    "            print(\"Not enough documents to sample from, please select smaller ratio or reduce the number of similar docs per query to select\")\n",
    "            return None\n",
    "        all_non_relevant[query_id] = random.sample(list(non_relevant_doc_ids), num_docs_to_sample)\n",
    "\n",
    "    return all_non_relevant\n"
   ],
   "id": "687cb964bb805e0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_docs(top_k, non_relevant_doc_ids_by_query, num_docs_to_sample):\n",
    "    \"\"\"\n",
    "    General evaluation function for both random and hard negatives.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    print(f\"There are {top_k} relevant docs and {num_docs_to_sample} non-relative docs per query\")\n",
    "    \n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['Question', 'Ground Truth Answer', 'LLM Answer', 'Match', 'Relevant Documents', 'Non-Relevant Documents']\n",
    "        writer.writerow(header)\n",
    "    \n",
    "    for query in tqdm(queries, desc=\"Evaluating queries\", unit=\"query\"):\n",
    "        top_k_similar_doc_ids_of_query = set(top_k_similar_docs_by_query[query.id()].keys())\n",
    "        non_relevant_doc_ids_of_query = non_relevant_doc_ids_by_query[query.id()]\n",
    "        is_match = evaluate_query(query, non_relevant_doc_ids_of_query, top_k_similar_doc_ids_of_query)\n",
    "        if is_match:\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "    return matches, mismatches"
   ],
   "id": "28b5e803d0114d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "top_k = 3 # number of relevant docs to include\n",
    "\n",
    "top_k_similar_docs_by_query = get_top_k_from_retrieved(similar_docs_by_query, top_k)"
   ],
   "id": "a93936e5220f7493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:17:44.322492Z",
     "start_time": "2024-12-21T14:37:13.941592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate with random contexts\n",
    "# relevant_ratio = 0.5 # ratio of relevant docs / total docs e.g. 3 / 10 (would add 7 more non-relevant documents)\n",
    "num_random_docs_to_sample = 3 #int(top_k * (1 - relevant_ratio) / relevant_ratio)\n",
    "output_file = f'llm_random_results-{top_k}-{num_random_docs_to_sample}.csv'\n",
    "\n",
    "random_doc_ids_by_query = compute_all_non_relevant(num_random_docs_to_sample, False)\n",
    "matches, mismatches = evaluate_with_docs(top_k, random_doc_ids_by_query, num_docs_to_sample=num_random_docs_to_sample)\n",
    "print(\"Matches:\", matches)\n",
    "print(\"Mismatches:\", mismatches)"
   ],
   "id": "a2e7eb386ee84b93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving non-relevant docs for each query: 100%|██████████| 1200/1200 [02:27<00:00,  8.15query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 relevant docs and 3 non-relative docs per query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   1%|          | 10/1200 [00:12<25:04,  1.26s/query]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating queries: 100%|██████████| 1200/1200 [38:03<00:00,  1.90s/query] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 549\n",
      "Mismatches: 651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "num_hard_negatives_to_sample = 3 #int(top_k * (1 - relevant_ratio) / relevant_ratio)\n",
    "output_file = f'llm_hard_neg_results-{top_k}-{num_hard_negatives_to_sample}.csv'\n",
    "# Evaluate with hard negatives\n",
    "hard_negative_doc_ids_by_query = compute_all_non_relevant(num_hard_negatives_to_sample, True)\n",
    "matches, mismatches = evaluate_with_docs(top_k, hard_negative_doc_ids_by_query, num_docs_to_sample=num_hard_negatives_to_sample)\n",
    "print(\"Matches:\", matches)\n",
    "print(\"Mismatches:\", mismatches)"
   ],
   "id": "582d7aa98394eeb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:54:11.799136Z",
     "start_time": "2024-12-21T18:54:11.642940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# Replace 'input.csv' with the path to your CSV file\n",
    "input_file = 'llm_results.csv'\n",
    "output_file_1 = 'llm_random_results.csv'\n",
    "output_file_2 = 'llm_hard_neg_results.csv'\n",
    "# Adjust encoding as necessary\n",
    "file_encoding = 'utf-8'  # Try 'utf-8' or 'latin1' if utf-8 doesn't work\n",
    "\n",
    "try:\n",
    "    # Read the input CSV with the specified encoding\n",
    "    with open(input_file, 'r', encoding=file_encoding) as infile:\n",
    "        reader = list(csv.reader(infile))\n",
    "        \n",
    "        # Extract the header\n",
    "        header = reader[0]\n",
    "        \n",
    "        # Divide rows into two parts\n",
    "        part1_rows = reader[1:1201]\n",
    "        part2_rows = reader[1201:]\n",
    "        \n",
    "        # Write the first part\n",
    "        with open(output_file_1, 'w', newline='', encoding=file_encoding) as outfile1:\n",
    "            writer = csv.writer(outfile1)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(part1_rows)\n",
    "        \n",
    "        # Write the second part\n",
    "        with open(output_file_2, 'w', newline='', encoding=file_encoding) as outfile2:\n",
    "            writer = csv.writer(outfile2)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(part2_rows)\n",
    "    \n",
    "    print(f\"CSV has been split into {output_file_1} and {output_file_2}.\")\n",
    "\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error reading the file: {e}\")\n"
   ],
   "id": "8aa555ff0e4c8d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV has been split into llm_random_results.csv and llm_hard_neg_results.csv.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "25639be0d7fa46ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:35:12.893625Z",
     "start_time": "2024-12-19T13:35:03.183402Z"
    }
   },
   "source": [
    "def evaluate_with_random_contexts(relevant_docs_by_query, relevant_ratio=0.5):\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for query in queries:\n",
    "        query_id = query.id()\n",
    "        top_k_relevant_docs_ids = list(relevant_docs_by_query[query_id].keys())\n",
    "        top_k_relevant_docs = [evidence for evidence in corpus if evidence.id in top_k_relevant_docs_ids]\n",
    "        \n",
    "        # Get non relevant docs from qrels (where qrels[query_id][doc_id] = 0)\n",
    "        non_relevant_doc_ids = {doc_id for doc_id, relevance in qrels.get(query_id, {}).items() if relevance == 0}\n",
    "        random_non_relevant_docs = [evidence for evidence in corpus if evidence.id in non_relevant_doc_ids]\n",
    "        num_random_docs = int(len(top_k_relevant_docs) * (1 - relevant_ratio) / relevant_ratio)\n",
    "        # TODO think on how to not get duplicates\n",
    "        # random_docs = random_non_relevant_docs[:num_random_docs]\n",
    "        random_docs = random.sample(random_non_relevant_docs, min(num_random_docs, len(non_relevant_doc_ids)))\n",
    "        \n",
    "        # Combine relevant and random non-relevant docs\n",
    "        combined_docs = top_k_relevant_docs + random_docs\n",
    "        random.shuffle(combined_docs)\n",
    "        \n",
    "        # Pass to LLM for evaluation\n",
    "        answer = query_llm(query.text(), combined_docs)\n",
    "        ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "        \n",
    "        # Evaluate the answer\n",
    "        if answer is not None and ground_truth_answer.lower() in answer.lower():\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "        \n",
    "        results[query_id] = answer\n",
    "        if i == 5:\n",
    "            break\n",
    "        i+=1\n",
    "        print(\"Answer:\", answer)\n",
    "        print(\"Ground truth answer:\", ground_truth_answer)\n",
    "    return matches, mismatches\n",
    "\n",
    "matches, mismatches = evaluate_with_random_contexts(response)\n",
    "print(\"Matches:\", matches)\n",
    "print(\"Mismatches:\", mismatches)\n",
    "# print(\"Matching Ratio:\", matches / len(queries))\n",
    "# print(\"Mismatching Ratio:\", mismatches / len(queries))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: None\n",
      "Ground truth answer: Małgorzata Braunek\n",
      "Answer: [/INST] Yes I will reason and generate the answer </s><s>[INST] Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\n",
      "Context: \n",
      "Answer: [/INST]  The film \"The Mask of Fu Manchu\" was released in 1932, while \"Blind Shaft\" does not appear to be a known film. Therefore, the answer to the question is \"The Mask of Fu Manchu\".\n",
      "\n",
      "Final Answer: The Mask of Fu Manchu (1932)\n",
      "Ground truth answer: The Mask Of Fu Manchu\n",
      "Answer: [/INST] Yes I will reason and generate the answer </s><s>[INST] Question: When did John V, Prince Of Anhalt-Zerbst's father die?\n",
      "Context: \n",
      "Answer: [/INST]  Based on the information provided in the context, John V, Prince of Anhalt-Zerbst's father, Christian August, died in 1747.\n",
      "\n",
      "Therefore, the final answer to the question is:\n",
      "\n",
      "1747\n",
      "Ground truth answer: 12 June 1516\n",
      "Answer: The director of \"Wearing Velvet Slippers Under a Golden Umbrella\" won the Golden Palm Award at the Cannes Film Festival.\n",
      "Ground truth answer: Myanmar Motion Picture Academy Awards\n",
      "Answer: Ronnie Rocket was born in the United States.\n",
      "Ground truth answer: Missoula, Montana\n",
      "Matches: 1\n",
      "Mismatches: 5\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "4e75723788b2bd77",
   "metadata": {},
   "source": [
    "### Task 4\n",
    " In this step, we will adopt a more principled approach to sample negative documents to be used as input to the RAG setup. Using a retrieval model, sample hard negatives from the collection for the\n",
    "current query instead of random documents to inject as noise. hard negatives are documents that are related and close to the query in the vector space but do not help answer the question. This can be sampled by retrieving documents not in the list of ground truth documents for a query as measure by dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90f91ef62c7a9019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:10.266634Z",
     "start_time": "2024-12-13T10:36:10.249692Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_with_hard_negatives(relevant_docs_by_query, relevant_ratio=0.5):\n",
    "    matches = 0\n",
    "    mismatches = 0\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for query in queries:\n",
    "        query_id = query.id()\n",
    "        top_k_relevant_docs_ids = list(relevant_docs_by_query[query_id].keys())\n",
    "        top_k_relevant_docs = [evidence for evidence in corpus if evidence.id in top_k_relevant_docs_ids]\n",
    "        \n",
    "        # Get non relevant docs from qrels (where qrels[query_id][doc_id] = 0)\n",
    "        non_relevant_doc_ids = {doc_id for doc_id, relevance in qrels.get(query_id, {}).items() if relevance == 0}\n",
    "        \n",
    "        #hard negatives are the documents both in retrieved docs (they are similar) and non-relevant docs (since they are not actually relevant)\n",
    "        hard_negatives_all = [evidence for evidence in corpus if evidence.id in top_k_relevant_docs_ids and evidence.id in non_relevant_doc_ids]\n",
    "        \n",
    "        num_hard_negatives = int(len(top_k_relevant_docs) * (1 - relevant_ratio) / relevant_ratio)\n",
    "        # TODO change this if evaluate_with_random_contexts is changed for the \"no duplicates todo\" part\n",
    "        hard_negatives_ratioed = hard_negatives_all[:num_hard_negatives]\n",
    "        \n",
    "        # Combine relevant and hard negative docs\n",
    "        combined_docs = top_k_relevant_docs + hard_negatives_ratioed\n",
    "        random.shuffle(combined_docs)\n",
    "        \n",
    "        # Pass to LLM for evaluation\n",
    "        answer = query_llm(query.text(), combined_docs)\n",
    "        ground_truth_answer = question_ground_truth_answer_map[query.id()]\n",
    "        \n",
    "        # Evaluate the answer\n",
    "        if answer is not None and ground_truth_answer.lower() in answer.lower():\n",
    "            matches += 1\n",
    "        else:\n",
    "            mismatches += 1\n",
    "        \n",
    "        results[query_id] = answer\n",
    "        if i == 5:\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "evaluate_with_hard_negatives(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93e8c6a3e03131",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train a retrieval model using ADORE [14]. ADORE is optimized with hard negatives in a dense retrieval setup. Hence, it may be able to discern more relevant documents from large collections and lead to improved downstream answer generation performance. Using this retriever, retrieve relevant contexts followed by answer generation using LLMs. Compare it to the baseline performance of contriever based LLM QA mentioned in step 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b87a7ff1f833c9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:36:13.851897Z",
     "start_time": "2024-12-13T10:36:13.847910Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe5da82911b029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
